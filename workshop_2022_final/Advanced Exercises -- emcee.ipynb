{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a model to data\n",
    "\n",
    "(adapted from the `emcee` [documentation](https://emcee.readthedocs.io/))\n",
    "\n",
    "[`emcee`](https://ui.adsabs.harvard.edu/abs/2013PASP..125..306F/) is a popular package for doing MCMC analysis. It uses an affine invariant sampler (see [this](https://ui.adsabs.harvard.edu/abs/2010CAMCS...5...65G/) paper if you want to know more) which is better than the Metropolis-Hastings sampler (read more [here](https://ui.adsabs.harvard.edu/abs/2005AJ....129.1706F/)) at exploring parameter space with strong degeneracies. It is plug and play once you write a log likelihood (or $\\chi^2$ function). This tutorial takes you through fitting a line to data using three diferent methods. At the end I will guide you through using `emcee` to fit the gaussian from Day 3. \n",
    "\n",
    "### Important:\n",
    "\n",
    "Before running this notebook you will need to install a few packages. Run the following lines in Terminal to install `emcee` (the MCMC hammer), `corner` (a nice tool for plotting the output of MCMC fits), and `tqdm` (a handy progress bar):\n",
    "```\n",
    "conda install -c conda-forge emcee\n",
    "python -m pip install corner\n",
    "conda install tqdm\n",
    "```\n",
    "You may need to restart your kernel or notebook server after installing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're reading this right now then you're probably interested in using\n",
    "emcee to fit a model to some noisy data.\n",
    "On this page, I'll demonstrate how you might do this in the simplest\n",
    "non-trivial model that I could think of: fitting a line to data when you\n",
    "don't believe the error bars on your data.\n",
    "The interested reader should check out [Hogg, Bovy & Lang (2010)](https://arxiv.org/abs/1008.4686) for a much more complete discussion of how\n",
    "to fit a line to data in The Real World™ and why MCMC might come in handy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The generative probabilistic model\n",
    "\n",
    "When you approach a new problem, the first step is generally to write down the\n",
    "*likelihood function* (the probability of a dataset given the model\n",
    "parameters).\n",
    "This is equivalent to describing the generative procedure for the data.\n",
    "In this case, we're going to consider a linear model where the quoted\n",
    "uncertainties are underestimated by a constant fractional amount.\n",
    "You can generate a synthetic dataset from this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "# Choose the \"true\" parameters.\n",
    "m_true = -0.9594\n",
    "b_true = 4.294\n",
    "f_true = 0.534\n",
    "\n",
    "# Generate some synthetic data from the model.\n",
    "N = 50\n",
    "x = np.sort(10 * np.random.rand(N))\n",
    "yerr = 0.1 + 0.5 * np.random.rand(N)\n",
    "y = m_true * x + b_true\n",
    "y += np.abs(f_true * y) * np.random.randn(N)\n",
    "y += yerr * np.random.randn(N)\n",
    "\n",
    "plt.errorbar(x, y, yerr=yerr, fmt=\".k\", capsize=0)\n",
    "x0 = np.linspace(0, 10, 500)\n",
    "plt.plot(x0, m_true * x0 + b_true, \"k\", alpha=0.3, lw=3)\n",
    "plt.xlim(0, 10)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true model is shown as the thick grey line and the effect of the\n",
    "underestimated uncertainties is obvious when you look at this figure.\n",
    "The standard way to fit a line to these data (assuming independent Gaussian\n",
    "error bars) is linear least squares. We'll use the routine you used previously "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize as opt\n",
    "\n",
    "def line(x, m, b):\n",
    "    return m*x + b\n",
    "\n",
    "bfpars, covar = opt.curve_fit(line, x, y, p0=[-1., 4.], sigma=yerr, absolute_sigma=True)\n",
    "m_ls, b_ls = bfpars\n",
    "\n",
    "print(\"Least-squares estimates:\")\n",
    "print(\"m = {0:.3f} ± {1:.3f}\".format(m_ls, np.sqrt(covar[0, 0])))\n",
    "print(\"b = {0:.3f} ± {1:.3f}\".format(b_ls, np.sqrt(covar[1, 1])))\n",
    "\n",
    "plt.errorbar(x, y, yerr=yerr, fmt=\".k\", capsize=0)\n",
    "plt.plot(x0, m_true * x0 + b_true, \"k\", alpha=0.3, lw=3, label=\"truth\")\n",
    "plt.plot(x0, m_ls*x0 + b_ls, \"--k\", label=\"LS\")\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlim(0, 10)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This figure shows the least-squares estimate of the line parameters as a dashed line.\n",
    "This isn't an unreasonable result but the uncertainties on the slope and\n",
    "intercept seem a little small (because of the small error bars on most of the\n",
    "data points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum likelihood estimation\n",
    "\n",
    "The least squares solution found in the previous section is the maximum\n",
    "likelihood result for a model where the error bars are assumed correct,\n",
    "Gaussian and independent.\n",
    "We know, of course, that this isn't the right model.\n",
    "Unfortunately, there isn't a generalization of least squares that supports a\n",
    "model like the one that we know to be true.\n",
    "Instead, we need to write down the likelihood function and numerically\n",
    "optimize it.\n",
    "In mathematical notation, the correct likelihood function is:\n",
    "\n",
    "$$\n",
    "    \\ln\\,p(y\\,|\\,x,\\sigma,m,b,f) =\n",
    "    -\\frac{1}{2} \\sum_n \\left[\n",
    "        \\frac{(y_n-m\\,x_n-b)^2}{s_n^2}\n",
    "        + \\ln \\left ( 2\\pi\\,s_n^2 \\right )\n",
    "    \\right]\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "    s_n^2 = \\sigma_n^2+f^2\\,(m\\,x_n+b)^2 \\quad .\n",
    "$$\n",
    "\n",
    "This likelihood function is simply a Gaussian where the variance is\n",
    "underestimated by some fractional amount:  $f$.\n",
    "In Python, you would code this up as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#theta is a single variable/vector that encompasses all our parameters\n",
    "def log_likelihood(theta, x, y, yerr):\n",
    "    #unpack theta\n",
    "    m, b, log_f = theta\n",
    "    \n",
    "    model = m * x + b\n",
    "    sigma2 = yerr**2 + model**2 * np.exp(2 * log_f)\n",
    "    \n",
    "    return -0.5 * np.sum((y - model) ** 2 / sigma2 + np.log(sigma2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code snippet, you'll notice that we're using the logarithm of $f$\n",
    "instead of $f$ itself for reasons that will become clear in the next section.\n",
    "For now, it should at least be clear that this isn't a bad idea because it\n",
    "will force $f$ to be always positive.\n",
    "A good way of finding this numerical optimum of this likelihood function is to\n",
    "use the [scipy.optimize](https://docs.scipy.org/doc/scipy/reference/optimize.html) module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "#make a negative log likelihood since we want to maximize but optimize only has a minimize function\n",
    "nll = lambda *args: -log_likelihood(*args)\n",
    "\n",
    "#initial guess\n",
    "initial = np.array([m_true, b_true, np.log(f_true)]) + 0.1 * np.random.randn(3)\n",
    "\n",
    "soln = opt.minimize(nll, initial, args=(x, y, yerr))\n",
    "m_ml, b_ml, log_f_ml = soln.x\n",
    "\n",
    "print(\"Maximum likelihood estimates:\")\n",
    "print(\"m = {0:.3f}\".format(m_ml))\n",
    "print(\"b = {0:.3f}\".format(b_ml))\n",
    "print(\"f = {0:.3f}\".format(np.exp(log_f_ml)))\n",
    "\n",
    "plt.errorbar(x, y, yerr=yerr, fmt=\".k\", capsize=0)\n",
    "plt.plot(x0, m_true * x0 + b_true, \"k\", alpha=0.3, lw=3, label=\"truth\")\n",
    "plt.plot(x0, m_ls*x0 + b_ls, \"--k\", label=\"LS\")\n",
    "plt.plot(x0, m_ml*x0 + b_ml, \":k\", label=\"ML\")\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlim(0, 10)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth noting that the optimize module *minimizes* functions whereas we\n",
    "would like to maximize the likelihood.\n",
    "This goal is equivalent to minimizing the *negative* likelihood (or in this\n",
    "case, the negative *log* likelihood).\n",
    "In this figure, the maximum likelihood (ML) result is plotted as a dotted black line—compared to\n",
    "the true model (grey line) and linear least-squares (LS; dashed line).\n",
    "That looks better!\n",
    "\n",
    "The problem now: how do we estimate the uncertainties on *m* and *b*?\n",
    "What's more, we probably don't really care too much about the value of *f* but\n",
    "it seems worthwhile to propagate any uncertainties about its value to our\n",
    "final estimates of *m* and *b*.\n",
    "This is where MCMC comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marginalization & uncertainty estimation\n",
    "\n",
    "This isn't the place to get into the details of why you might want to use MCMC\n",
    "in your research but it is worth commenting that a common reason is that you\n",
    "would like to marginalize over some \"nuisance parameters\" and find an estimate\n",
    "of the posterior probability function (the distribution of parameters that is\n",
    "consistent with your dataset) for others.\n",
    "MCMC lets you do both of these things in one fell swoop!\n",
    "You need to start by writing down the posterior probability function (up to a\n",
    "constant):\n",
    "\n",
    "$$\n",
    "    p (m,b,f\\,|\\,x,y,\\sigma) \\propto p(m,b,f)\\,p(y\\,|\\,x,\\sigma,m,b,f) \\quad .\n",
    "$$\n",
    "\n",
    "We have already, in the previous section, written down the likelihood function\n",
    "\n",
    "$$\n",
    "p(y\\,|\\,x,\\sigma,m,b,f)\n",
    "$$\n",
    "\n",
    "so the missing component is the \"prior\" function\n",
    "\n",
    "$$\n",
    "p(m,b,f) \\quad .\n",
    "$$\n",
    "\n",
    "This function encodes any previous knowledge that we have about the\n",
    "parameters: results from other experiments, physically acceptable ranges, etc.\n",
    "It is necessary that you write down priors if you're going to use MCMC because\n",
    "all that MCMC does is draw samples from a probability distribution and you\n",
    "want that to be a probability distribution for your parameters.\n",
    "This is important: **you cannot draw parameter samples from your likelihood\n",
    "function**.\n",
    "This is because a likelihood function is a probability distribution **over\n",
    "datasets** so, conditioned on model parameters, you can draw representative\n",
    "datasets (as demonstrated at the beginning of this exercise) but you cannot\n",
    "draw parameter samples.\n",
    "\n",
    "In this example, we'll use uniform (so-called \"uninformative\") priors on $m$,\n",
    "$b$ and the logarithm of $f$.\n",
    "For example, we'll use the following conservative prior on $m$:\n",
    "\n",
    "$$\n",
    "p(m) = \\left \\{\\begin{array}{ll}\n",
    "        1 / 5.5 \\,, & \\mbox{if}\\,-5 < m < 1/2 \\\\\n",
    "        0 \\,, & \\mbox{otherwise}\n",
    "    \\end{array}\n",
    "    \\right .\n",
    "$$\n",
    "\n",
    "In code, the log-prior is (up to a constant):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prior(theta):\n",
    "    m, b, log_f = theta\n",
    "    #if the parameters are within acceptable values\n",
    "    if -5.0 < m < 0.5 and 0.0 < b < 10.0 and -10.0 < log_f < 1.0:\n",
    "        return 0.0\n",
    "    #if outside\n",
    "    return -np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, combining this with the definition of ``log_likelihood`` from above, the full\n",
    "log-probability function is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_probability(theta, x, y, yerr):\n",
    "    lp = log_prior(theta)\n",
    "    #if the prior tells us the parameters are bad, don't bother calling the likelihood function\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + log_likelihood(theta, x, y, yerr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all this setup, it's easy to sample this distribution using emcee.\n",
    "We'll start by initializing the walkers in a tiny Gaussian ball around the\n",
    "maximum likelihood result (I've found that this tends to be a pretty good\n",
    "initialization in most cases) and then run 5,000 steps of MCMC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "\n",
    "#use optimize fit as an initial guess with a small width and 32 'walkers'\n",
    "pos = emcee.utils.sample_ball([m_ml,b_ml,log_f_ml],[1e-4,1e-4,1e-4],32)\n",
    "#you could also do this manually as follows\n",
    "#pos = soln.x + 1e-4 * np.random.randn(32, 3)\n",
    "nwalkers, ndim = pos.shape\n",
    "\n",
    "# define the sampler, providing it your log_probability function and any other arguements that function takes\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_probability, args=(x, y, yerr))\n",
    "sampler.run_mcmc(pos, 5000, progress=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what the sampler has done.\n",
    "A good first step is to look at the time series of the parameters in\n",
    "the chain.\n",
    "The samples can be accessed using the {func}`EnsembleSampler.get_chain` method.\n",
    "This will return an array\n",
    "with the shape `(5000, 32, 3)` giving the parameter values for each walker\n",
    "at each step in the chain.\n",
    "The figure below shows the positions of each walker as a function of the\n",
    "number of steps in the chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, figsize=(10, 7), sharex=True)\n",
    "samples = sampler.get_chain()\n",
    "labels = [\"m\", \"b\", \"log(f)\"]\n",
    "for i in range(ndim):\n",
    "    ax = axes[i]\n",
    "    ax.plot(samples[:, :, i], \"k\", alpha=0.3)\n",
    "    ax.set_xlim(0, len(samples))\n",
    "    ax.set_ylabel(labels[i])\n",
    "    ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "\n",
    "axes[-1].set_xlabel(\"step number\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, the walkers start in small distributions around the\n",
    "maximum likelihood values and then they quickly wander and start exploring the\n",
    "full posterior distribution.\n",
    "In fact, after fewer than 50 steps, the samples seem pretty well \"burnt-in\".\n",
    "That is a hard statement to make quantitatively, but we can look at an estimate\n",
    "of the integrated autocorrelation time (see the {ref}`autocorr` tutorial for more details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = sampler.get_autocorr_time()\n",
    "print(tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests that only about 40 steps are needed for the chain to \"forget\" where it started.\n",
    "It's not unreasonable to throw away a few times this number of steps as \"burn-in\".\n",
    "Let's discard the initial 100 steps, thin by about half the autocorrelation time (15 steps), and flatten the chain so that we have a flat list of samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_samples = sampler.get_chain(discard=100, thin=15, flat=True)\n",
    "print(flat_samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Now that we have this list of samples, let's make one of the most useful plots\n",
    "you can make with your MCMC results: *a corner plot*.\n",
    "You'll need the [corner.py module](http://corner.readthedocs.io) but\n",
    "once you have it, generating a corner plot is as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "\n",
    "fig = corner.corner(\n",
    "    flat_samples, labels=labels, truths=[m_true, b_true, np.log(f_true)]\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corner plot shows all the one and two dimensional projections of the\n",
    "posterior probability distributions of your parameters.\n",
    "This is useful because it quickly demonstrates all of the covariances between\n",
    "parameters.\n",
    "Also, the way that you find the marginalized distribution for a parameter or\n",
    "set of parameters using the results of the MCMC chain is to project the\n",
    "samples into that plane and then make an N-dimensional histogram.\n",
    "That means that the corner plot shows the marginalized distribution for each\n",
    "parameter independently in the histograms along the diagonal and then the\n",
    "marginalized two dimensional distributions in the other panels.\n",
    "\n",
    "Another diagnostic plot is the projection of your results into the space of\n",
    "the observed data.\n",
    "To do this, you can choose a few (say 100 in this case) samples from the chain\n",
    "and plot them on top of the data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = np.random.randint(len(flat_samples), size=100)\n",
    "for ind in inds:\n",
    "    sample = flat_samples[ind]\n",
    "    plt.plot(x0, np.dot(np.vander(x0, 2), sample[:2]), \"C1\", alpha=0.1)\n",
    "plt.errorbar(x, y, yerr=yerr, fmt=\".k\", capsize=0)\n",
    "plt.plot(x0, m_true * x0 + b_true, \"k\", label=\"truth\")\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlim(0, 10)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leaves us with one question: which numbers should go in the abstract?\n",
    "There are a few different options for this but my favorite is to quote the\n",
    "uncertainties based on the 16th, 50th, and 84th percentiles of the samples in\n",
    "the marginalized distributions.\n",
    "To compute these numbers for this example, you would run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Math\n",
    "\n",
    "for i in range(ndim):\n",
    "    mcmc = np.percentile(flat_samples[:, i], [16, 50, 84])\n",
    "    q = np.diff(mcmc)\n",
    "    txt = \"\\mathrm{{{3}}} = {0:.3f}_{{-{1:.3f}}}^{{{2:.3f}}}\"\n",
    "    txt = txt.format(mcmc[1], q[0], q[1], labels[i])\n",
    "    display(Math(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now its your turn\n",
    "\n",
    "We're going to fit the same gaussian data/curve you did on Day 3 but with MCMC.\n",
    "\n",
    "### Copy and paste your solution to Question 1 below to read in the \"test_spectrum.txt\" data:\n",
    "You could also plot it again if you want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a more simple log likelihood function than above, just negative one half times chi sqared:\n",
    "    \n",
    "$$\n",
    "    \\ln\\,p(y\\,|\\,x,\\sigma_x,f_0, \\mu, \\sigma) =\n",
    "    -\\frac{1}{2} \\chi^2\n",
    "$$\n",
    "\n",
    "Where $\\chi^2$ is the squared difference between the data and the model, scaled by the error:\n",
    "$$\n",
    "    \\chi^2 = \\sum_n \\left[\n",
    "        \\left(\\frac{\\mathrm{data}_n-\\mathrm{model}_n}{\\mathrm{error}_n}\\right)^2\n",
    "    \\right] = \\sum_n \\left[\n",
    "        \\left(\\frac{y_n-G(x_n,f_0,\\mu,\\sigma)}{\\sigma_{x_n}}\\right)^2 \\right]\n",
    "$$\n",
    "    \n",
    "and $G$ is our gaussian function from before: $G(x_n,f_0,\\mu,\\sigma)=f_0*\\exp\\left(\\frac{-(x_n-\\mu)^2}{(2\\sigma^2)}\\right) $\n",
    "## 1) Define your log likelihood function.\n",
    "### First copy paste your gaussian function from Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then write another function to calculate $\\chi^2$ using your Gaussian function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally write your log likelihood function. The first parameter should be theta, a vector of parameters. I've started this for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(theta):\n",
    "    mu, sigma, f0 = theta\n",
    "    \n",
    "    # your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Prior\n",
    "\n",
    "Now we need to define our prior. Use the framework written above to return 0 if the values are ok and -$\\infty$ if they are outside reasonable bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prior(theta):\n",
    "    mu, sigma, f0 = theta\n",
    "    \n",
    "    # your code here\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will use basically the same combined log probability function from a bove, copy that here and adapt it to our functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_probability(theta):\n",
    "    \n",
    "    # your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will set up the sampler and run it\n",
    "Like above, we can use our optmize results as our starting guess.\n",
    "### Copy the relavent code from above to initialize the walkers and run the sampler\n",
    "Fill in the initial guess with your value from `scipy.optimize` and guess some (fiarly narrow) widths. \n",
    "\n",
    "Hint: we don't need the `args` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy the relavent code to view the chains and check the autocorrelation time\n",
    "\n",
    "Look for the burn in. If the chains look weird try playing around with the initial guess and widths above and re-run it. If too many walkers start in \"bad\" areas of parameter space they won't converge.\n",
    "\n",
    "Like we did above you can check the autocorrelation time to make sure the chains have converged and learn how much to cut out for your analyisis (you don't want your initial guess to affect your analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we want to see what parameters emcee gave us\n",
    "## Copy and adapt the relavent code to trim the chain and make a corner plot\n",
    "\n",
    "Since we don't know what the true values are you could use the `scipy.optimize` values if you want to compare the two methods. Try using `show_titles=True` to print the median values and $\\pm1\\sigma$ error bars for each parameter. Also try using `quantiles=[0.16,0.5,0.84]` to show the median and $\\pm1\\sigma$ values of the histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy and adapt the code used to create the final example plot from above and plot the data, best fit model, and a sample of the walkers all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally print out the median and $\\pm1\\sigma$ values\n",
    "Hint: since $f_0$ is so small you may need to multiply by $10^{18}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
